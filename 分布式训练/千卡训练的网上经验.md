## 简介

因为本人没有训练过千卡的经验，以上都是看网上别人的经验的。

主要来源于知乎问题：《如何判断候选人有没有千卡GPU集群的训练经验？》


## 为什么千卡训练是困难的？

千卡训练和八卡训练的区别是—显卡多了一百多倍。这意味着什么呢？通信时间增加FB">故障概率增加这俩问题都很好理解。时间上，PyTorch内部支持NCCL/Gloo/MPI三个通信后端（请务必使用NCCL。其中AllReduce操作会会根据具体硬件配置走Ring AllReduce和Tree AllReduce。Ring的时间复杂度是O(p n)，Tree的时间复杂度是O(\log p n)。就算是理论上128节点也比单节点慢至少七倍，实践当中跨节点通讯要远比单节点慢得多。

故障上，一个节点出问题的概率是p，128个节点就是1-(1-p)^128。也就是说如果一个操作在一个训练当中的出错概率是1%，那么在128节点当中的出错概率就是72.37%。此外，随着规模的增大，许多问题都会变得难以忍受。比如数据增强要花0.1s，一亿条数据就是278个小时（当然这只是胡拆的一个数字，实际有各种机制所以不会有这么大影响。



## 如何使用一千张卡训练？### 如何提高计算效率？

这件事情其实是一个case by case的事情。因为通信、计算速度啥的受硬件影响更多。而每一个集群的硬件拓扑都是不一样的。同样是A100集群，我全DGX节点，每一张A100都是SXM接口并配一块儿专属的IB网卡。你一个小破普惠服务器插8张PCI-E A100，IB卡一个节点只给一张。那咱俩遇到的问题就完全不是一个问题。因此，要讨论如何提高训练效率、减少训练耗时，我们首先要了解训练耗时在哪里。那么，一个训练步的耗时在哪里呢？需要谨记，没有profile的优化是没有意义的。你可能会说，forward backward sync。很好，这说明你了解PyTorch的基本流程。不过现实当中要复杂得多。

1. dataset读取数据，构建输出

2. dataloader collate数据，进行数据预处理

3. 模型forward

4. 计算输出loss 

5. compute模型backward计算梯度

6. 模型sync梯度

7. 优化器step更新权重

8. 打印log

当然这是可以无限细分下去的，但一般这些就够了。需要注意的是，除了4-7的耗时是真耗时，其他都需要通过异步操作来盖掉。这也是我们的优化目标。异步执行在PyTorch的dataloader、CUDA和分布式当中都存在。前者可以通过设置num_workers和prefetch_count为0来关闭，后两者可以通过cuda.synchornize和dist.barrier来执行手动同步。在profile时，我们需要首先需要测整个step的时长。然后再在每次测量前执行手动同步来计算每个部分的时长。如果前者的总耗时等于后者4-7的耗时之和，那么通常不需要执行任何操作。但这种情况在千卡操作中几乎不可能发生。第6步通信往往需要耗费大量时间。因此，我们还需要进一步优化通信。


#### 计算-通信重叠

在PyTorch当中，梯度的通信和反向传播是交叠进行的。也就是说，每完成一层的梯度计算，都会立即触发当前层的同步。实现起来也很简单，每个进程在完成自己第k层的梯度计算后都会触发一个钩子来给计数器+1s。当计数器达到进程数是开火进行梯度通信。有很多同学在计算梯度过程中遇到过RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.错误，这就是因为有的模块没有参与计算loss，导致梯度同步卡住了。需要注意，当find_unused_parameters=True时，PyTorch分布式使用nn.Module.__init__当中定义sub-module的反向顺序来作为梯度桶的构建顺序。因此，确保模块定义和调用的顺序一致对于高效训练来说很重要。



#### 梯度合桶

尽管理论上来说，同步发生的越及时，重合度越高，性能越好。但实际上每次发起通信都是有overhead的。因此，现实当中梯度同步并不是越多越好越快越好。为此，PyTorch引入了梯度合桶机制，通过把多个Tensor装在一个桶里再通信桶来减少通信次数从而减少总耗时。合桶的Buffer Size等等参数往往需要针对硬件和模型来调整从而取得最好的通信效果。PyTorch的默认参数是从0.x时代祖传下来的，这一参数通常都需要调节。

#### 梯度累加

当你做完所有操作之后，惊喜的发现TMD怎么同步时间还是单节点的好几倍。这其实是正常情况……实际上超过256卡的训练想要把通信盖掉就是一件不可能的事情。你说老师我看FB论文说他们256卡就是线性提升啊…那这里不得不提的一个策略就是梯度累加了。梯度累加会执行k次forward+backward之后再执行优化器步进。这有很多好处，首先对于大模型batch size通常不能开多大，梯度累加可以提升等效batch sizs。其次累加期间的backwars不需要通信梯度，加快了训练速度。

#### 收敛，收敛，收敛

模型训着训着发散了几乎是每个训大模型的人都会遇到的问题。输出和loss只要有nan果断丢掉。梯度先clip by value再clip by norm都是常规操作。哦对了，还有初始化……关于大模型收敛性的论文有一堆，此处不再赘述。

#### 弹性训练

每k分钟，系统会做一次AllReduce来统计存活进程数，然后选举出一个主进程。主进程会计算好每个进程的rank和local rank进行broadcast。所有进程每次forward开始时向主进程发送一个心跳包来汇报状态。主进程会根据心跳包来确定这一个step参与同步的机器有多少。




作者：你的真实姓名
链接：https://www.zhihu.com/question/650979052/answer/3501160453
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


## 资源trade-off

如果你的千卡GPU集群采用是高速网络通信的话，那么因为网络通信训练中断的概率还是挺大的（1次/2周），所以需要经常保存 Checkpoint，方便中断的时候及时回滚，多久 Checkpoint 一次是一种 Trade-Off。另外，GPU服务器也会存在掉卡或者状态异常的情况（1-2台/天），一般大模型的整个训练过程都会保持GPU数量不变（不然会影响数据并行组的个数，虽然Megatron支持自适应的分片模式，但一般还是建议保持一致），所以会预留一部分GPU当作 Backup，预留多少也是一种 Trade-Off

作者：简枫
链接：https://www.zhihu.com/question/650979052/answer/3455112382
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


